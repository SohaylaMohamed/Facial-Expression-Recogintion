<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Expression Evaluation</title>

  <!-- Bootstrap core CSS -->
  <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark static-top">
    <div class="container">
      <a class="navbar-brand" href="../home.html">CS435 Introduction to Deep Learning</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item active">
            <a class="nav-link" href="../home.html">Home
              <span class="sr-only">(current)</span>
            </a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../about.html">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../contact.html">Contact</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Content -->
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h1 class="mt-5">Expression Evaluation</h1>
        <ul class="list-unstyled">
          <li>Rita Samir #25</li>
          <li>Sohayla Mohammed #31</li>
        </ul>
      </div>
    </div>


    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Problem Statement</h2>
		<p>
			Humans use different forms of communications such as speech, hand gestures and emotions. Being able to understand oneâ€™s emotions and the encoded feelings is an important factor for an appropriate and correct understanding. Facial expression is one of the most important components in daily communications of human beings. It is generated by movements of facial muscles. While different people have different kinds of facial expressions caused by their own expressive styles or personalities, many studies show that there are several types of basic expressions shared by different peoples with different cultural and ethnic backgrounds.
		</p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Dataset</h2>
        <h5 class="mt-5">Dataset #1: CK+.</h5>
        <p>
          The Extended CohnKanade (CK+) database is the most extensively used  laboratory-controlled database for evaluating FER systems. CK+ contains 593 video sequences from 123 subjects. The sequences vary in duration from 10 to 60 frames and show a shift from a neutral facial expression to the peak expression. Among these videos, 327 sequences from 118 subjects are labeled with seven basic expression labels (anger, contempt, disgust, fear, happiness, sadness, and surprise) based on the Facial Action Coding System (FACS). Because CK+ does not provide specified training, validation and test sets, the algorithms evaluated on this database are not uniform. For static-based methods, the most common data selection method is to extract the last one to three frames with peak formation and the first frame (neutral face) of each sequence. Then, the subjects are divided into n groups for person-independent n-fold cross-validation experiments, where commonly selected values of n are 5, 8 and 10.
        </p>

        <br/> <!-- Empty Line before the image -->
        <div class="img-container" align="center"> <!-- Block parent element -->
            <img src="resources/images/CK-examples.jpeg" class="img-fluid text-center">
        </div>
        <br/> <!-- Empty Line after the image -->

        <h5 class="mt-5">Dataset #2: FER2013.</h5>

        <p>
          In Facial Expression Evaluation, one of the biggest challenges is to achieve good accuracy on wild, spontaneous shot images where the dataset portrays real-life scenarios.
          A dataset that follows that creteria is FER2013 dataset; It consists of 35886 portraits and 7 emotions ('anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise', 'neutral').
          The portraits illustrate variabilities in illumination, age, pose, expression intensity, and occlusions that occur under realistic conditions.
          They are represented as a gray scale images of size 48x48x1.
		</p>

		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/dataset-samples.jpg" class="img-fluid text-center">
    	</div>
    	<br/> <!-- Empty Line after the image -->
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Input/Output Examples</h2>

        <p>
			For FER2013 dataset.
		</p>
    <p>
			Dataset Examples :
		</p>
		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/dataset-examples.png" class="img-fluid text-center">
    	</div>
      <br/> <!-- Empty Line after the image -->
      <p>
        Personalised Examples :
      </p>
      <br/> <!-- Empty Line before the image -->
        <div class="img-container" align="center"> <!-- Block parent element -->
            <img src="resources/images/personalised-exaples.jpg" class="img-fluid text-center">
        </div>
        <br/> <!-- Empty Line after the image -->
      </div>
    </div>


    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">State of the art</h2>

        <p>
			State here state of the art models and their accuracies:

		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/State-of-the-art.png" class="img-fluid text-center">
    	</div>
    	<br/> <!-- Empty Line after the image -->
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Orignial Model from Literature</h2>

        <p>
           The original model is based on the Dexpression(2015) paper.  It is a deep Convolutional Neural Network architecture which consists of four parts.
           The first part automatically preprocesses the data. This begins with Convolution 1, which applies 64 different filters. The next layer is Pooling 1, which down-samples the images and then they are normalized by LRN 1. The next steps are the two FeatEx (Parallel Feature Extraction Block) blocks. The features extracted by these blocks are forwarded to a fully connected layer, which uses them to classify the input into the different emotions.
        </p>

		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/original-model.png" class="img-fluid text-center">
    	</div>
    	<br/> <!-- Empty Line after the image -->
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Proposed Updates</h2>

		<h5 class="mt-5">Update #1: Used the original model with and ensemble of deep CNNs and ResNet.</h5>
		<p>
      Made the original model a part of an ensemble with other DCNN and ResNet models. The ensemble accuracy was determined using three different techniques.
      <ul>
        <li>Majority Voting.</li>
        <li>Average (Sum).</li>
        <li>Weighted average(Validation accuracy as weight, Individual testing accuracy as weight).</li>

      </ul>
		</p>
		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/ensemple-results.png" class="img-fluid text-center">
    	</div>
    	<br/> <!-- Empty Line after the image -->

		<h5 class="mt-5">Update #2: Used HOG with DCNN model.</h5>
		<p>
      After doing some research, we've come upon a new concept for us which is HOG; It acts as a conventional feature extraction method. Which we merged with the DCNN model before the classification layer. HOG showed great results with FER2013 as an individual model.		</p>
		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/HOG.png" class="img-fluid text-center">
    	</div>
      <br/> <!-- Empty Line after the image -->
      
      <h5 class="mt-5">Update #3: Used HOG with DCNN model in an ensemble with another DCNN model.</h5>
      <p>
        Although HOG showed great individual results we wanted to further optimize its results by using it with another model from a previous Ensemble model we've made which also showed great individual results.		</p>
      <br/> <!-- Empty Line before the image -->
        <div class="img-container" align="center"> <!-- Block parent element -->
            <img src="resources/images/HOG.png" class="img-fluid text-center">
        </div>
        <div class="img-container" align="center"> <!-- Block parent element -->
          <img src="resources/images/cnn2.png" class="img-fluid text-center">
        </div>
        <br/> <!-- Empty Line after the image -->
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Results</h2>
        <h5 class="mt-5">In our evaluation we have used a different split than other models, and achieved better accuracies.</h5>
        <p>
			1 . Original model deployed. 
		</p>

		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/orig-deploy.png" class="img-fluid text-center">
    	</div>
      <br/> <!-- Empty Line after the image -->
      
      <p>
        2 . Ensemble method #1 
      </p>
  
      <br/> <!-- Empty Line before the image -->
        <div class="img-container" align="center"> <!-- Block parent element -->
            <img src="resources/images/ensemple-results.png" class="img-fluid text-center">
        </div>
        <br/> <!-- Empty Line after the image -->

        <p>
          3 . HOG/CNN2/Ensemble
        </p>
    
        <br/> <!-- Empty Line before the image -->
          <div class="img-container" align="center"> <!-- Block parent element -->
              <img src="resources/images/hog-ensemble.png" class="img-fluid text-center">
          </div>
          <br/> <!-- Empty Line after the image -->

      </div>
    </div>


    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Technical report</h2>

        <p>
			Here you will detail the details related to training, for example:
		</p>

	 	<ul>
		  <li>Python/Tensorflow/Keras.</li>
		  <li>Google Colab GP as architecture.</li>
		  <li>Training time : ~ 20 min per run</li>
		  <li>Number of epochs : ~60-100</li>
		  <li>Time per epoch : differs from each model ~10-25 seconds</li>
		</ul> 
      </div>
    </div>

	<div class="row">
	  <div class="col-lg-12 text-left">
	    <h2 class="mt-5">Conclusion</h2>

	    <p>
        As a conclusion we think that sometimes conventional methods can outperform DL ones. And combined together they can produce great results.
        <br/>
        For future work :
        <ul>
          <li>Introduce pose invariance, which depends on image frontalization. the current state of the art methods still struggles with keeping the expression intact after frontalization.</li>
          <li>Find a way to handle different obstacles with occlusions, illumination and so on.</li>
        </ul>
		</p>

	  </div>
	</div>

	<div class="row">
	  <div class="col-lg-12 text-left">
	    <h2 class="mt-5">References</h2>

		<ol>
      <li><a href="https://www.kaggle.com/gauravsharma99/fer-using-multiple-pipelines/notebook">FER using multiple pipelines</a></li>
      <li><a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">FER2013 dataset</a></li>

		  <li><a href="https://en.wikipedia.org/wiki/Facial_expression_databases?fbclid=IwAR1F8tdfZP0a0OZz6t2-oGpcW26meNYTA7WL-mNvfv14Za3jt-JHSaWoRcQ">Datasets for FER</a></li>
      <li><a href="https://arxiv.org/pdf/1509.05371v2.pdf">Dexpression paper</a></li>
      <li><a href="https://www.mdpi.com/2073-8994/11/10/1189/htm#:~:text=Facial%20Expression%20Recognition%20(FER)%2C,of%20recent%20advances%20in%20FER."> FER survey</a></li>
		</ol> 
	  </div>
	</div>

  </div>



  <!-- Bootstrap core JavaScript -->
  <script src="../vendor/jquery/jquery.slim.min.js"></script>
  <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

</body>

</html>
